{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# increase column number display in pandas\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "def read_csv():\n",
    "    # read csv and parse dates column to datetime\n",
    "    df = pd.read_csv(\"loan.csv\", parse_dates=['issue_d'])\n",
    "    return df\n",
    "\n",
    "def make_train_test(df):\n",
    "    # split data to train and test\n",
    "    sorted_df = df[df['loan_status'] != 'Current'].reset_index(drop=True)\n",
    "    sorted_df[\"loan_status\"] = sorted_df[\"loan_status\"].map({\"Fully Paid\": 0, \"Charged Off\": 1})\n",
    "    X = sorted_df.drop('loan_status', axis=1)\n",
    "    y = sorted_df['loan_status']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    X_train.reset_index(drop=True,inplace=True)\n",
    "    X_test.reset_index(drop=True,inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "\n",
    "df = read_csv()\n",
    "X_train, X_test, y_train, y_test = make_train_test(df)\n",
    "\n",
    "# # check data by sweetviz\n",
    "# my_report = sv.analyze(X_train)\n",
    "# my_report.show_html() # Default arguments will generate to \"SWEETVIZ_REPORT.html\"\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base():    \n",
    "    def fit(self, input_df):\n",
    "        return self.transform(input_df)\n",
    "        \n",
    "    def transform(self, input_df):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeleteUninformableColumn(Base):\n",
    "    def __init__(self):\n",
    "        self.deleted_columns = []\n",
    "        \n",
    "    def fit(self, input_df):\n",
    "        # delete columns which has more than 50% missing values\n",
    "        origin_columns = set(input_df.columns)\n",
    "        deleted_df = input_df.dropna(thresh=len(df)/2, axis=1)\n",
    "        \n",
    "        # check columns which has same values of 90% data and delete it.\n",
    "        for col in deleted_df.columns:\n",
    "            if deleted_df[col].value_counts().iloc[0] > len(deleted_df)*0.9:\n",
    "                output_df = deleted_df.drop(col, axis=1)\n",
    "        self.deleted_columns=set(output_df.columns)-origin_columns\n",
    "        print(f\"delete columns: {self.deleted_columns}\")\n",
    "        return self.transform(input_df)\n",
    "    \n",
    "    def transform(self, input_df):\n",
    "        output_df = input_df.drop(self.deleted_columns, axis=1)\n",
    "        return output_df\n",
    "    \n",
    "delete_colums = DeleteUninformableColumn()\n",
    "X_train = delete_colums.fit(X_train)\n",
    "X_test = delete_colums.transform(X_test)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessFeatures(Base):\n",
    "    def transform_term(self, input_df):\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df['converted_term'] = input_df['term'].str.replace('months', '').astype(int)\n",
    "        return output_df\n",
    "\n",
    "    def transform_int_rate(self, input_df):\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df['converted_int_rate'] = input_df['int_rate'].str.replace('%', '').astype(float)\n",
    "        return output_df\n",
    "    \n",
    "    def transform_revol_util(self, input_df):\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df['converted_revol_util'] = input_df['revol_util'].str.replace('%', '').astype(float)\n",
    "        return output_df\n",
    "    \n",
    "    def transform(self, input_df):\n",
    "        input_df[\"term\"] = self.transform_term(input_df)\n",
    "        input_df[\"int_rate\"] = self.transform_int_rate(input_df)\n",
    "        input_df[\"revol_util\"] = self.transform_revol_util(input_df)\n",
    "        return input_df\n",
    "\n",
    "preprocess = PreprocessFeatures()\n",
    "X_train = preprocess.fit(X_train)\n",
    "X_test = preprocess.transform(X_test)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "class ordinal_encoding_based_on_frequency_encoding(Base):\n",
    "    def __init__(self,col):\n",
    "        self.col = col\n",
    "    \n",
    "    def fit(self, input_df):\n",
    "        output_df = pd.DataFrame()\n",
    "        value_counted = input_df[self.col].value_counts().sort_values()\n",
    "        mapping_dict= {value:i for i,value in enumerate(value_counted.index)}\n",
    "        mapping = [{\"col\":self.col,\"mapping\":mapping_dict}]\n",
    "        self.encoder_ = ce.OrdinalEncoder(mapping=mapping)\n",
    "        output_df = self.encoder_.fit_transform(input_df[self.col])\n",
    "        return output_df.add_prefix(f\"ordinal_\")\n",
    "    \n",
    "    def transform(self, input_df):\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df = self.encoder_.transform(input_df[self.col])\n",
    "        return output_df.add_prefix(f\"ordinal_\")\n",
    "\n",
    "# test = ordinal_encoding_based_on_frequency_encoding(\"grade\")\n",
    "# test.fit(X_train)\n",
    "# test.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked predictionによりテキストをメタ特徴量化する。\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import ElasticNet,LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class text_out_of_fold_prediction(Base):\n",
    "  def __init__(self, col):\n",
    "    self.nfolds = 5\n",
    "    self.seed = 71\n",
    "    self.col = col\n",
    "    self.models = []\n",
    "    self.regression = False\n",
    "    self.scores = []\n",
    "    \n",
    "\n",
    "    \n",
    "  def fit(self, input_df, y):\n",
    "    x = input_df[self.col].fillna(\"nan\")\n",
    "    # self.vectorizer_ = TfidfVectorizer(max_features=100)\n",
    "    self.vectorizer_ = CountVectorizer(max_features=100)\n",
    "    x = self.vectorizer_.fit_transform(x)\n",
    "    self.binerizer_ = Binarizer()\n",
    "    x = self.binerizer_.fit_transform(x)\n",
    "    \n",
    "    if self.regression:\n",
    "      fold = KFold(n_splits=self.nfolds, random_state=self.seed, shuffle=True)\n",
    "    else:\n",
    "      fold = StratifiedKFold(n_splits=self.nfolds, random_state=self.seed, shuffle=True)\n",
    "    oof_train = np.zeros(len(input_df))\n",
    "    # stacked prediction\n",
    "    for i, (train_idx, valid_idx) in enumerate(fold.split(input_df[self.col], y)):\n",
    "      train_x, train_y = x[train_idx], y.loc[train_idx]\n",
    "      valid_x, valid_y = x[valid_idx], y.loc[valid_idx]\n",
    "      \n",
    "      if self.regression:\n",
    "        clf = ElasticNet(random_state=self.seed)\n",
    "        clf.fit(train_x, train_y)\n",
    "        pred_y = clf.predict(valid_x)\n",
    "        score = mean_squared_error(valid_y, pred_y)\n",
    "      else:\n",
    "        clf = LogisticRegression(penalty='elasticnet',solver='saga',random_state=self.seed, max_iter=10000, C=1.0, l1_ratio=0.5, class_weight='balanced')\n",
    "        clf.fit(train_x, train_y)\n",
    "        pred_y = clf.predict_proba(valid_x)[:,1]\n",
    "        score = roc_auc_score(valid_y, pred_y)  \n",
    "      print(f'CV Score of Fold_{i} is {score}')\n",
    "      self.models.append(clf)\n",
    "      self.scores.append(score)\n",
    "      oof_train[valid_idx]= pred_y\n",
    "      \n",
    "    print(f\"mean score is {np.mean(self.scores)}\")\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[self.col] = oof_train\n",
    "    return output_df.add_prefix(\"predicted_\")\n",
    "        \n",
    "  def transform(self, input_df):\n",
    "    x = input_df[self.col].fillna(\"nan\")\n",
    "    x = self.vectorizer_.transform(x)\n",
    "    x = self.binerizer_.transform(x)\n",
    "    oof_test = np.zeros(len(input_df))\n",
    "    if self.regression:\n",
    "      for clf in self.models:\n",
    "        oof_test += clf.predict(x)\n",
    "    else:\n",
    "      for clf in self.models:\n",
    "        oof_test += clf.predict_proba(x)[:,1]\n",
    "    oof_test /= self.nfolds\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[self.col] = oof_test\n",
    "    return output_df.add_prefix(\"predicted_\")\n",
    "\n",
    "test = text_out_of_fold_prediction(\"desc\")\n",
    "test.fit(X_train, y_train)\n",
    "test.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストの重要堂やカウントを整理する\n",
    "class text_check_insight():\n",
    "    def __init__(self, col):\n",
    "        self.col = col\n",
    "        self.regression =False\n",
    "        self.seed = 1\n",
    " \n",
    "    def create_words_summary(self, clf, x):\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df[\"features\"] = self.vectorizer_.get_feature_names_out()\n",
    "        output_df[\"wordcounts\"] = (np.array(x.sum(axis=0)).flatten())\n",
    "        output_df[\"coefs\"] = clf.coef_.flatten()\n",
    "        return output_df.sort_values(by=\"coefs\").reset_index(drop=True)\n",
    "    \n",
    "    def __call__(self, input_df, y):\n",
    "        x = input_df[self.col].fillna(\"nan\")\n",
    "        self.vectorizer_ = CountVectorizer(max_features=1000)\n",
    "        x = self.vectorizer_.fit_transform(x)\n",
    "        self.binerizer_ = Binarizer()\n",
    "        x = self.binerizer_.fit_transform(x)\n",
    "        \n",
    "        if self.regression:\n",
    "            clf = ElasticNet(random_state=self.seed)\n",
    "        else:\n",
    "            clf = LogisticRegression(penalty='elasticnet',solver='saga',random_state=self.seed, max_iter=10000, C=1.0, l1_ratio=0.5, class_weight='balanced')\n",
    "        clf.fit(x, y)\n",
    "        return self.create_words_summary(clf, x)\n",
    "\n",
    "    \n",
    "test = text_check_insight(\"desc\")\n",
    "df_word_summary = test(X_train, y_train)\n",
    "df_word_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ワードクラウドで可視化する。\n",
    "# 閾値を設けてある一定の頻度のものだけ表示にしてもいいかも\n",
    "# 今後Stopword除去も配慮する\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dictionary where keys are words and values are corresponding wordcounts\n",
    "wordcounts = dict(zip(df_word_summary['features'], df_word_summary['wordcounts']))\n",
    "\n",
    "# Define a color function\n",
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    coef = df_word_summary[df_word_summary['features'] == word]['coefs'].values[0]\n",
    "    if coef > 0:\n",
    "        return \"hsl(0, 70%%, %d%%)\" % (50 + coef * 100) # adjust these numbers to change color\n",
    "    else:\n",
    "        return \"hsl(200, 70%%, %d%%)\" % (50 - coef * 100) # adjust these numbers to change color\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, color_func=color_func,\n",
    "                      prefer_horizontal=1.0).generate_from_frequencies(wordcounts)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all features\n",
    "class RunBlocks(Base):\n",
    "    def __init__(self):\n",
    "        self.feature_blocks = [*[ordinal_encoding_based_on_frequency_encoding(col) for col in\n",
    "                    [\"grade\", \"sub_grade\",\"emp_length\",\"home_ownership\",\"verification_status\",\n",
    "                     \"purpose\",\"zip_code\",\"addr_state\",]]]\n",
    "        self.stacked_predict_feature = [*[text_out_of_fold_prediction(col) for col in [\"desc\",\"title\"]]]\n",
    "        self.use_original_values = [\"loan_amnt\",\"funded_amnt\",\"funded_amnt_inv\",\"term\",\n",
    "                                \"int_rate\",\"installment\",\"annual_inc\"]\n",
    "        \n",
    "    def fit(self,input_df, df_y):\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df = input_df[self.use_original_values]\n",
    "                \n",
    "        for block in self.feature_blocks:\n",
    "            output_i = block.fit(input_df)\n",
    "            assert len(input_df) == len(output_i), block\n",
    "            output_df = pd.concat([output_df,output_i],axis=1)\n",
    "        \n",
    "        for block in self.stacked_predict_feature:\n",
    "            output_i = block.fit(input_df,df_y)\n",
    "            assert len(input_df) == len(output_i), block\n",
    "            output_df = pd.concat([output_df,output_i],axis=1)\n",
    "        return output_df\n",
    "    \n",
    "    def transform(self,input_df):\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df = input_df[self.use_original_values]\n",
    "        \n",
    "        for block in self.feature_blocks:\n",
    "            output_i = block.transform(input_df)\n",
    "            assert len(input_df) == len(output_i), block\n",
    "            output_df = pd.concat([output_df,output_i],axis=1)\n",
    "            \n",
    "        for block in self.stacked_predict_feature:\n",
    "            output_i = block.transform(input_df)\n",
    "            assert len(input_df) == len(output_i), block\n",
    "            output_df = pd.concat([output_df,output_i],axis=1)\n",
    "        return output_df\n",
    "        \n",
    "run_blocks = RunBlocks()\n",
    "df_train = run_blocks.fit(X_train, y_train)\n",
    "df_test = run_blocks.transform(X_test)\n",
    "df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d280a3d7820d8a7cd68b3ac1b6c938b4612fc133bd228f2e7ab871064ed1cce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
