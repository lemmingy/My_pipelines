{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sensor_data.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1gkgXU9rEQkJ3TZHrwxkWDtB6jy5B50dX",
      "authorship_tag": "ABX9TyN/7Ymk0vC2ZyfzbfAjqVbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lemmingy/My_pipelines/blob/main/Sensor_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEquTi7k6LeI"
      },
      "source": [
        "# Top"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqQVjcmL5Ino"
      },
      "source": [
        "pip install tsfresh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoeU6QNROL-R"
      },
      "source": [
        "センサーデータの変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3D7T1tl5Cek"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tsfresh import extract_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjR7lAti4jgt"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Kaggle/Library/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7AZOmSf4gpm"
      },
      "source": [
        "#サンプル用のDataFrame\n",
        "df=pd.DataFrame()\n",
        "df[\"spectrum_id\"]=['000da4633378740f1ee8',\n",
        " '000ed1a5a9fe0ad2b7dd',\n",
        " '0016e3322c4ce0700f9a',\n",
        " '00256bd0f8c6cf5f59c8',\n",
        " '003483ee5ae313d37590']\n",
        "df[\"spectrum_filename\"]=['b2e223339f4abce9b400.dat',\n",
        " 'e2f150a503244145e7ce.dat',\n",
        " '3d58b7ccaee157979cf0.dat',\n",
        " 'ed3641184d3b7c0ae703.dat',\n",
        " '4c63418d39f86dfab9bb.dat']\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5xgIr9AQl-i"
      },
      "source": [
        "folder_name=\"/content/drive/My Drive/Colab Notebooks/Kaggle/Library/Input_data/Sensor_data/spectrum_raw/\"\n",
        "\n",
        "#Tsfreshに入れやすい形にSpectrumデータをロード\n",
        "def concat_spectrum(df_train, folder_name):\n",
        "\n",
        "  spec_df=pd.DataFrame()\n",
        "  for x in df_train.spectrum_filename:\n",
        "      df_tmp = pd.read_csv(folder_name+x, \n",
        "                            sep='\\t', \n",
        "                            header=None)\n",
        "      \n",
        "      #おまけ。513データの物があったため。\n",
        "      if len(df_tmp)==512:\n",
        "          df_tmp.drop(index=0,inplace=True)\n",
        "      df_tmp[0]=x\n",
        "      spec_df=pd.concat([spec_df, df_tmp])\n",
        "  return spec_df\n",
        "\n",
        "spec_df=concat_spectrum(df, folder_name)\n",
        "spec_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOlDJ_G4OLhl"
      },
      "source": [
        "#Tsfreshによる特徴量の作成\n",
        "df_features = extract_features(spec_df, column_id=0)\n",
        "df_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8bsBPk6E-N"
      },
      "source": [
        "# For NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9-i2v5LksR6"
      },
      "source": [
        "#スペクトラムファイルを横持ちで呼び込み\n",
        "\n",
        "def concat_spectrum2(df_train, folder_name):\n",
        "  spec_df2=pd.DataFrame()\n",
        "  for x in df_train.spectrum_filename:\n",
        "      df_tmp = pd.read_csv(folder_name+x, \n",
        "                                sep='\\t', \n",
        "                                header=None)\n",
        "      if len(df_tmp[0])==512:\n",
        "          df_tmp.drop(index=0,inplace=True)\n",
        "      df_tmp.reset_index(drop=True, inplace=True)\n",
        "      df_tmp=df_tmp[[1]].T\n",
        "      df_tmp[\"spectrum_filename\"]=x\n",
        "      spec_df2=pd.concat([spec_df2, df_tmp])\n",
        "  return spec_df2\n",
        "\n",
        "concat_spectrum2(df, folder_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJplVINl4NdS"
      },
      "source": [
        "# Conv1Dのシンプルモデル\n",
        "def mlp_raw():\n",
        "    model = Sequential()\n",
        "\n",
        "    kernel=8\n",
        "    model.add(Conv1D(filters=64, kernel_size=(kernel), strides=1, padding='same',\n",
        "                     activation='relu', kernel_initializer='he_normal', input_shape=(None, 1)))\n",
        "    model.add(Conv1D(filters=64, kernel_size=(kernel), strides=1, padding='same',\n",
        "                     activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=(kernel), strides=1, padding='same',\n",
        "                     activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid'))#256\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Conv1D(filters=64, kernel_size=(kernel), strides=1, padding='same',\n",
        "                     activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=(kernel), strides=1, padding='same',\n",
        "                     activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=(kernel), strides=1, padding='same',\n",
        "                     activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid'))#128\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Conv1D(filters=64, kernel_size=(kernel), strides=1, padding='same',\n",
        "                     activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=(kernel), strides=1, padding='same',\n",
        "                     activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=(kernel), strides=1, padding='same',\n",
        "                     activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid'))#64\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=32, activation='relu',kernel_initializer='he_normal'))    \n",
        "    model.add(Dense(1,activation=\"sigmoid\"))\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[\"binary_crossentropy\"])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2w93zSs4sQz"
      },
      "source": [
        "# NNにFitする\n",
        "def nn_raw_fit_pred(df_train_x,df_train_y,df_test_tmp,model_descript):\n",
        "    oof_train = np.zeros((df_train_x.shape[0],1))\n",
        "    oof_test = np.zeros((df_test_tmp.shape[0],1))\n",
        "    scores=[]\n",
        "    def expand_dim(x):\n",
        "        x = np.expand_dims(x,axis=2)\n",
        "        return x\n",
        "    print(model_descript)\n",
        "    df_test_tmp=expand_dim(df_test_tmp)\n",
        "   \n",
        "    for i, (train_ix, test_ix) in enumerate(kfold.split(df_train_x,df_train_y)):\n",
        "        X_tr, y_tr = df_train_x.iloc[train_ix].values,df_train_y.iloc[train_ix].values\n",
        "        X_val, y_val = df_train_x.iloc[test_ix].values, df_train_y.iloc[test_ix].values\n",
        "        \n",
        "\n",
        "        \n",
        "        X_tr=expand_dim(X_tr)\n",
        "        X_val=expand_dim(X_val)\n",
        "      \n",
        "        print(X_val.shape)\n",
        "        print(df_test_tmp.shape)\n",
        "        \n",
        "        filepath = \"mlp_best_model.hdf5\" \n",
        "\n",
        "        es = EarlyStopping(patience=5, mode='min', verbose=1) \n",
        "\n",
        "        checkpoint = ModelCheckpoint(monitor='val_loss',filepath=filepath, save_best_only=True, mode='auto') \n",
        "\n",
        "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.1, verbose=1, mode='min')\n",
        "        print(X_tr.shape)\n",
        "\n",
        "        model = mlp_raw(X_tr)\n",
        "\n",
        "        history = model.fit(X_tr, y_tr, batch_size=32, epochs=100, validation_data=(X_val, y_val), \n",
        "                            callbacks=[es, checkpoint, reduce_lr_loss], verbose=100)\n",
        "\n",
        "        y_pred = model.predict(X_val, batch_size=32)\n",
        "        oof_train[test_ix] = y_pred\n",
        "\n",
        "        score=log_loss (y_val, y_pred)\n",
        "        print('CV log_loss Score of Fold_%d is %f' % (i, score))\n",
        "\n",
        "        score=average_precision_score (y_val, y_pred)\n",
        "        print('CV average_precision_score of Fold_%d is %f' % (i, score))\n",
        "\n",
        "        scores.append(score)\n",
        "        test_pred=model.predict(df_test_tmp, batch_size=32)\n",
        "        oof_test += test_pred\n",
        "\n",
        "\n",
        "    print(np.mean(scores),\"+/-\",np.std(scores))\n",
        "    \n",
        "    oof_test /= NFOLDS\n",
        "    return oof_train,oof_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr8G7gzl7iiA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}